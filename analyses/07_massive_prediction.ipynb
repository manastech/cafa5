{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 7\n",
    "\n",
    "## Massive prediction of test set\n",
    "\n",
    "Once our models are trained, we're ready to predict for each protein in the test set, the GO term probability for each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more time, to avoid the memory to go full, we have developed a pipeline that make the predictions for all the proteins for a single term that we will execute in a separated thread for each term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the pipeline to the working environment\n",
    "import sys\n",
    "sys.path.append('../scripts/')\n",
    "\n",
    "from singleTermPredPipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some imports \n",
    "from manas_cafa.bio.protein import Protein\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow memory growth for the GPU\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just have predict to predict, for each model, for all the test set proteins. To avoid fulfilling all the GPU memory, we will take batches of 2500 proteins to predict at a same time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/models/12/0005912.h5\n",
      "PREDICTING /data/models/12/0005912.h5\n",
      "Predicting for 2500 proteins\n",
      "79/79 [==============================] - 1s 2ms/step\n",
      "Predicting for 2500 proteins\n",
      "79/79 [==============================] - 0s 2ms/step\n",
      "Predicting for 2499 proteins\n",
      "79/79 [==============================] - 0s 2ms/step\n",
      "Predicting for 2498 proteins\n",
      "78/78 [==============================] - 0s 2ms/step\n",
      "Predicting for 2500 proteins\n",
      "79/79 [==============================] - 0s 2ms/step\n",
      "Predicting for 2499 proteins\n",
      "79/79 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "for path in Path('/data/models/').rglob('*.h5'):\n",
    "    print(path)\n",
    "    \n",
    "    try:\n",
    "        p = multiprocessing.Process(target=singleTermPredPipeline( path, results_path=\"/data/models/predictions.tsv\", batch_size=2500 ))\n",
    "        p.start()\n",
    "        p.join()\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
